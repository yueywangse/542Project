{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed6becb4",
   "metadata": {},
   "source": [
    "### RQ2 ‚Äì Agent identity, change complexity, and merge probability\n",
    "\n",
    "Goal of RQ2:\n",
    "\n",
    "- We want to see **how PR size / complexity** (lines changed, files touched)\n",
    "- and **which AI agent** (Codex, Copilot, Devin, Cursor, Claude Code)\n",
    "- together affect the **probability that a pull request is merged**.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Load the PR table and the commit-detail table.\n",
    "2. Build PR-level complexity features (total lines changed, files changed).\n",
    "3. Join them back to get a `df_rq2` dataset.\n",
    "4. Fit a **logistic regression** model to predict `merged_flag`.\n",
    "5. Use **10-fold cross-validation** + a held-out test set to evaluate performance.\n",
    "6. Interpret coefficients to answer RQ2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0c67d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# scikit-learn core tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Just for nicer console output\n",
    "def create_divider():\n",
    "    print('-' * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de508e3",
   "metadata": {},
   "source": [
    "### 1. Load main pull request table\n",
    "\n",
    "First, we load the main `pull_request.parquet` table.  \n",
    "This gives us PR-level metadata:\n",
    "\n",
    "- PR id, agent name, state (open/closed), timestamps, repo id, etc.\n",
    "- It **does not** contain complexity features like additions/deletions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e68990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pull_requests shape: (33596, 14)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "pull_requests columns:\n",
      "Index(['id', 'number', 'title', 'body', 'agent', 'user_id', 'user', 'state',\n",
      "       'created_at', 'closed_at', 'merged_at', 'repo_id', 'repo_url',\n",
      "       'html_url'],\n",
      "      dtype='object')\n",
      "----------------------------------------------------------------------------------------------------\n",
      "           id  number                                              title  \\\n",
      "0  3264933329    2911  Fix: Wait for all partitions in load_collectio...   \n",
      "1  3265118634       2                   „Éï„Ç°„Ç§„É´„Éë„ÇπÂèÇÁÖß„ÇíÁõ∏ÂØæ„Éë„Çπ„Å´Áµ±‰∏Ä„Åó„ÄÅdoc/„Åã„Çâdocs/„Å´Áµ±‰∏Ä   \n",
      "2  3265640341      30        Add build staleness detection for debug CLI   \n",
      "3  3265709660     205  feat: add comprehensive README screenshots wit...   \n",
      "4  3265782173   17625        chore: remove HashedPostStateProvider trait   \n",
      "\n",
      "                                                body        agent    user_id  \\\n",
      "0  ## Summary\\n\\nFixes an issue where `load_colle...  Claude_Code  108661493   \n",
      "1  ## ËÉåÊôØ\\n\\nÁèæÂú®„ÄÅÊú¨„Éó„É≠„Ç∏„Çß„ÇØ„Éà„Å´„Åä„ÅÑ„Å¶‰ª•‰∏ã„ÅÆ„Éë„ÇπÊßãÊàê„ÅÆ‰∏çÊï¥Âêà„ÅåÁîü„Åò„Å¶„ÅÑ„Åæ„ÅôÔºö\\n\\n...  Claude_Code   61827001   \n",
      "2  ## Summary\\r\\n\\r\\n  Implements comprehensive b...  Claude_Code       7475   \n",
      "3  ## Type of Change\\n\\n- [ ] üêõ `bug` - Bug fix (...  Claude_Code      80381   \n",
      "4  ## Summary\\r\\n\\r\\n#17545 \\r\\n\\r\\nRemove the un...  Claude_Code   47593288   \n",
      "\n",
      "         user   state            created_at             closed_at  \\\n",
      "0  weiliu1031  closed  2025-07-26T02:59:01Z  2025-07-29T07:01:20Z   \n",
      "1  cm-kojimat  closed  2025-07-26T04:56:55Z  2025-07-26T22:12:24Z   \n",
      "2        MSch  closed  2025-07-26T13:31:19Z  2025-07-26T13:37:22Z   \n",
      "3      sugyan  closed  2025-07-26T14:07:22Z  2025-07-26T14:45:30Z   \n",
      "4     adust09    open  2025-07-26T15:02:48Z                  None   \n",
      "\n",
      "              merged_at     repo_id  \\\n",
      "0                  None   191751505   \n",
      "1  2025-07-26T22:12:24Z  1025472321   \n",
      "2  2025-07-26T13:37:22Z   988488798   \n",
      "3  2025-07-26T14:45:30Z   999285986   \n",
      "4                  None   537233603   \n",
      "\n",
      "                                            repo_url  \\\n",
      "0    https://api.github.com/repos/milvus-io/pymilvus   \n",
      "1   https://api.github.com/repos/classmethod/tsumiki   \n",
      "2     https://api.github.com/repos/steipete/Peekaboo   \n",
      "3  https://api.github.com/repos/sugyan/claude-cod...   \n",
      "4      https://api.github.com/repos/paradigmxyz/reth   \n",
      "\n",
      "                                            html_url  \n",
      "0    https://github.com/milvus-io/pymilvus/pull/2911  \n",
      "1      https://github.com/classmethod/tsumiki/pull/2  \n",
      "2       https://github.com/steipete/Peekaboo/pull/30  \n",
      "3  https://github.com/sugyan/claude-code-webui/pu...  \n",
      "4     https://github.com/paradigmxyz/reth/pull/17625  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33596 entries, 0 to 33595\n",
      "Data columns (total 14 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          33596 non-null  int64 \n",
      " 1   number      33596 non-null  int64 \n",
      " 2   title       33596 non-null  object\n",
      " 3   body        33236 non-null  object\n",
      " 4   agent       33596 non-null  object\n",
      " 5   user_id     33596 non-null  int64 \n",
      " 6   user        33596 non-null  object\n",
      " 7   state       33596 non-null  object\n",
      " 8   created_at  33596 non-null  object\n",
      " 9   closed_at   31284 non-null  object\n",
      " 10  merged_at   24014 non-null  object\n",
      " 11  repo_id     33596 non-null  int64 \n",
      " 12  repo_url    33596 non-null  object\n",
      " 13  html_url    33596 non-null  object\n",
      "dtypes: int64(4), object(10)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "pull_requests = pd.read_parquet(\n",
    "    'hf://datasets/hao-li/AIDev/pull_request.parquet'\n",
    ")\n",
    "\n",
    "print('pull_requests shape:', pull_requests.shape)\n",
    "create_divider()\n",
    "print('pull_requests columns:')\n",
    "print(pull_requests.columns)\n",
    "create_divider()\n",
    "\n",
    "print(pull_requests.head())\n",
    "create_divider()\n",
    "pull_requests.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaf9632",
   "metadata": {},
   "source": [
    "### 2. The PR table is missing complexity columns\n",
    "\n",
    "The main PR table does **not** have:\n",
    "\n",
    "- `additions`\n",
    "- `deletions`\n",
    "- `files_changed`\n",
    "\n",
    "These fields live in the commit-related tables.  \n",
    "To build PR-level complexity, we now load:\n",
    "\n",
    "- `pr_commits.parquet`\n",
    "- `pr_commit_details.parquet`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e53890f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pr_id in columns? False\n",
      "agent_name in columns? False\n",
      "additions in columns? False\n",
      "deletions in columns? False\n",
      "files_changed in columns? False\n",
      "----------------------------------------------------------------------------------------------------\n",
      "pr_commits columns:\n",
      "Index(['sha', 'pr_id', 'author', 'committer', 'message'], dtype='object')\n",
      "----------------------------------------------------------------------------------------------------\n",
      "pr_commit_details columns:\n",
      "Index(['sha', 'pr_id', 'author', 'committer', 'message', 'commit_stats_total',\n",
      "       'commit_stats_additions', 'commit_stats_deletions', 'filename',\n",
      "       'status', 'additions', 'deletions', 'changes', 'patch'],\n",
      "      dtype='object')\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create a working copy of the pull_request table\n",
    "prs = pull_requests.copy()\n",
    "\n",
    "# Just to verify these features do NOT exist in this table\n",
    "key_cols = ['pr_id', 'agent_name', 'additions', 'deletions', 'files_changed']\n",
    "for c in key_cols:\n",
    "    print(c, 'in columns?', c in prs.columns)\n",
    "\n",
    "create_divider()\n",
    "\n",
    "# Load commit-level tables\n",
    "pr_commits = pd.read_parquet(\n",
    "    'hf://datasets/hao-li/AIDev/pr_commits.parquet'\n",
    ")\n",
    "\n",
    "pr_commit_details = pd.read_parquet(\n",
    "    'hf://datasets/hao-li/AIDev/pr_commit_details.parquet'\n",
    ")\n",
    "\n",
    "print('pr_commits columns:')\n",
    "print(pr_commits.columns)\n",
    "create_divider()\n",
    "\n",
    "print('pr_commit_details columns:')\n",
    "print(pr_commit_details.columns)\n",
    "create_divider()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0abcaf",
   "metadata": {},
   "source": [
    "### 3. Clean PR table and define merge flag\n",
    "\n",
    "We standardize column names and create a binary target:\n",
    "\n",
    "- `pr_id` ‚Äì PR identifier\n",
    "- `agent_name` ‚Äì which AI agent created this PR\n",
    "- `merged_flag` ‚Äì 1 if `merged_at` is non-null, 0 otherwise\n",
    "\n",
    "`merged_flag` is our **target variable** for logistic regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bee18e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PR-level columns (subset) ===\n",
      "        pr_id   agent_name   state            created_at  \\\n",
      "0  3264933329  Claude_Code  closed  2025-07-26T02:59:01Z   \n",
      "1  3265118634  Claude_Code  closed  2025-07-26T04:56:55Z   \n",
      "2  3265640341  Claude_Code  closed  2025-07-26T13:31:19Z   \n",
      "3  3265709660  Claude_Code  closed  2025-07-26T14:07:22Z   \n",
      "4  3265782173  Claude_Code    open  2025-07-26T15:02:48Z   \n",
      "\n",
      "              merged_at  merged_flag  \n",
      "0                  None            0  \n",
      "1  2025-07-26T22:12:24Z            1  \n",
      "2  2025-07-26T13:37:22Z            1  \n",
      "3  2025-07-26T14:45:30Z            1  \n",
      "4                  None            0  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "merged_flag value proportions:\n",
      "merged_flag\n",
      "1    0.715\n",
      "0    0.285\n",
      "Name: proportion, dtype: float64\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prs = prs.rename(columns={\n",
    "    'id': 'pr_id',        # PR identifier\n",
    "    'agent': 'agent_name' # AI agent\n",
    "})\n",
    "\n",
    "# Merge flag: 1 if merged_at is not null, else 0\n",
    "prs['merged_flag'] = prs['merged_at'].notna().astype(int)\n",
    "\n",
    "print('=== PR-level columns (subset) ===')\n",
    "print(prs[['pr_id', 'agent_name', 'state', 'created_at', 'merged_at', 'merged_flag']].head())\n",
    "\n",
    "create_divider()\n",
    "print('merged_flag value proportions:')\n",
    "print(prs['merged_flag'].value_counts(normalize=True).round(3))\n",
    "create_divider()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965363d2",
   "metadata": {},
   "source": [
    "### 4. Build PR-level complexity features\n",
    "\n",
    "Now we use `pr_commit_details` to build complexity for each PR.\n",
    "\n",
    "For each `pr_id`, we compute:\n",
    "\n",
    "- `total_additions` ‚Äì sum of additions across all files\n",
    "- `total_deletions` ‚Äì sum of deletions\n",
    "- `files_changed` ‚Äì number of unique files touched\n",
    "- `total_changes` ‚Äì sum of `changes` across files (lines changed)\n",
    "\n",
    "These become our **complexity features**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75d342a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== commit_details sample ===\n",
      "                                        sha       pr_id  \\\n",
      "0  2f9d54dda4f0c87c19e0bbeb9936f525d0587e16  3271196926   \n",
      "1  2f9d54dda4f0c87c19e0bbeb9936f525d0587e16  3271196926   \n",
      "2  2f9d54dda4f0c87c19e0bbeb9936f525d0587e16  3271196926   \n",
      "3  dbd1b5f129f7cffa5ce284d7255814c98bcc38a2  3271196926   \n",
      "4  c2659cfdedf666c8f14753d71664563c2a932b23  3271196926   \n",
      "\n",
      "                      author                  committer  \\\n",
      "0  devin-ai-integration[bot]  devin-ai-integration[bot]   \n",
      "1  devin-ai-integration[bot]  devin-ai-integration[bot]   \n",
      "2  devin-ai-integration[bot]  devin-ai-integration[bot]   \n",
      "3  devin-ai-integration[bot]  devin-ai-integration[bot]   \n",
      "4  devin-ai-integration[bot]  devin-ai-integration[bot]   \n",
      "\n",
      "                                             message  commit_stats_total  \\\n",
      "0  Add llms.txt compilation system for AI model d...               23008   \n",
      "1  Add llms.txt compilation system for AI model d...               23008   \n",
      "2  Add llms.txt compilation system for AI model d...               23008   \n",
      "3  Fix lint issues: remove unused variable and ap...                  35   \n",
      "4  Update llms.txt to follow official standard wi...               23035   \n",
      "\n",
      "   commit_stats_additions  commit_stats_deletions  \\\n",
      "0                   23008                       0   \n",
      "1                   23008                       0   \n",
      "2                   23008                       0   \n",
      "3                      18                      17   \n",
      "4                      89                   22946   \n",
      "\n",
      "                                 filename    status  additions  deletions  \\\n",
      "0  .github/workflows/compile-llms-txt.yml     added       38.0        0.0   \n",
      "1                docs/compile_llms_txt.py     added       47.0        0.0   \n",
      "2                                llms.txt     added    22923.0        0.0   \n",
      "3                docs/compile_llms_txt.py  modified       18.0       17.0   \n",
      "4                docs/compile_llms_txt.py  modified       51.0       36.0   \n",
      "\n",
      "   changes                                              patch  \n",
      "0     38.0  @@ -0,0 +1,38 @@\\n+name: Compile llms.txt\\n+\\n...  \n",
      "1     47.0  @@ -0,0 +1,47 @@\\n+import os\\n+from pathlib im...  \n",
      "2  22923.0                                               None  \n",
      "3     35.0  @@ -1,47 +1,48 @@\\n import os\\n from pathlib i...  \n",
      "4     87.0  @@ -3,45 +3,60 @@\\n \\n \\n def compile_llms_txt...  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "=== Complexity features (per PR) ===\n",
      "        pr_id  total_additions  total_deletions  files_changed  total_changes\n",
      "0  2756921963            848.0            344.0             15         1192.0\n",
      "1  2757103560            517.0            262.0             16          779.0\n",
      "2  2757124156              1.0              1.0              1            2.0\n",
      "3  2757125491              1.0              1.0              1            2.0\n",
      "4  2757179026           1633.0            169.0             15         1802.0\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Work from pr_commit_details\n",
    "cd = pr_commit_details.copy()\n",
    "\n",
    "print('=== commit_details sample ===')\n",
    "print(cd.head())\n",
    "create_divider()\n",
    "\n",
    "# Aggregate per PR\n",
    "complexity = (\n",
    "    cd.groupby('pr_id')\n",
    "      .agg(\n",
    "          total_additions=('additions', 'sum'),\n",
    "          total_deletions=('deletions', 'sum'),\n",
    "          files_changed=('filename', 'nunique'),\n",
    "          total_changes=('changes', 'sum'),\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "print('=== Complexity features (per PR) ===')\n",
    "print(complexity.head())\n",
    "create_divider()\n",
    "\n",
    "# Replace any remaining NaNs with 0 (safe for these counts)\n",
    "complexity[['total_additions', 'total_deletions', 'files_changed', 'total_changes']] = (\n",
    "    complexity[['total_additions', 'total_deletions', 'files_changed', 'total_changes']]\n",
    "    .fillna(0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e3cf98",
   "metadata": {},
   "source": [
    "### 5. Check missing values in PR table\n",
    "\n",
    "Some columns in `prs` have missing values, e.g. `merged_at` and `closed_at`.\n",
    "This is **expected**:\n",
    "\n",
    "- `merged_at` is missing when a PR was never merged.\n",
    "- We already encoded this into `merged_flag`.\n",
    "\n",
    "For RQ2, we only care that:\n",
    "\n",
    "- `pr_id`, `agent_name`, `merged_flag` are not missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "459ae4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in prs:\n",
      "merged_at      9582\n",
      "closed_at      2312\n",
      "body            360\n",
      "title             0\n",
      "agent_name        0\n",
      "number            0\n",
      "pr_id             0\n",
      "user              0\n",
      "user_id           0\n",
      "created_at        0\n",
      "state             0\n",
      "repo_id           0\n",
      "repo_url          0\n",
      "html_url          0\n",
      "merged_flag       0\n",
      "dtype: int64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Proportion missing per column (%):\n",
      "merged_at      28.52\n",
      "closed_at       6.88\n",
      "body            1.07\n",
      "title           0.00\n",
      "agent_name      0.00\n",
      "number          0.00\n",
      "pr_id           0.00\n",
      "user            0.00\n",
      "user_id         0.00\n",
      "created_at      0.00\n",
      "state           0.00\n",
      "repo_id         0.00\n",
      "repo_url        0.00\n",
      "html_url        0.00\n",
      "merged_flag     0.00\n",
      "dtype: float64\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Missing values in prs:')\n",
    "print(prs.isna().sum().sort_values(ascending=False))\n",
    "create_divider()\n",
    "\n",
    "print('Proportion missing per column (%):')\n",
    "print((prs.isna().mean() * 100).round(2).sort_values(ascending=False))\n",
    "create_divider()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02872075",
   "metadata": {},
   "source": [
    "### 6. Merge PR metadata with complexity and create log features\n",
    "\n",
    "We now join:\n",
    "\n",
    "- `prs` (PR metadata + agent + merged_flag)\n",
    "- `complexity` (per-PR size metrics)\n",
    "\n",
    "Then:\n",
    "\n",
    "- Fill missing `total_changes` and `files_changed` with 0 (for safety).\n",
    "- Create `log_total_changes = log(1 + total_changes)` to reduce skew.\n",
    "- Drop rows where key variables are missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "893254e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_rq2 shape: (33580, 19)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "        pr_id   agent_name  merged_flag  total_changes  files_changed\n",
      "0  3264933329  Claude_Code            0          396.0              3\n",
      "1  3265118634  Claude_Code            1           76.0             11\n",
      "2  3265640341  Claude_Code            1          407.0              5\n",
      "3  3265709660  Claude_Code            1          300.0             15\n",
      "4  3265782173  Claude_Code            0          221.0             21\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Merge PR metadata with complexity features\n",
    "df_rq2 = prs.merge(complexity, on='pr_id', how='inner')\n",
    "\n",
    "print('df_rq2 shape:', df_rq2.shape)\n",
    "create_divider()\n",
    "print(df_rq2[['pr_id', 'agent_name', 'merged_flag', 'total_changes', 'files_changed']].head())\n",
    "create_divider()\n",
    "\n",
    "# Basic cleaning\n",
    "df_rq2['total_changes'] = df_rq2['total_changes'].fillna(0)\n",
    "df_rq2['files_changed'] = df_rq2['files_changed'].fillna(0)\n",
    "\n",
    "# Log-transform complexity\n",
    "df_rq2['log_total_changes'] = np.log1p(df_rq2['total_changes'])\n",
    "\n",
    "# Drop any rows missing in model-relevant columns\n",
    "df_rq2 = df_rq2.dropna(\n",
    "    subset=['agent_name', 'merged_flag', 'log_total_changes', 'files_changed']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4592a2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in key columns:\n",
      "agent_name           0\n",
      "merged_flag          0\n",
      "log_total_changes    0\n",
      "files_changed        0\n",
      "dtype: int64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "merged_flag distribution (proportion):\n",
      "merged_flag\n",
      "1    0.715\n",
      "0    0.285\n",
      "Name: proportion, dtype: float64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Agent-level summary for RQ2:\n",
      "              n_prs  mean_total_changes  median_total_changes  merge_rate\n",
      "agent_name                                                               \n",
      "OpenAI_Codex  21793          800.888037                  74.0    0.826137\n",
      "Copilot        4967         5538.911013                 179.0    0.430642\n",
      "Devin          4822         2680.778308                 167.0    0.538158\n",
      "Cursor         1540         2861.625325                 271.5    0.652597\n",
      "Claude_Code     458         8646.951965                 691.0    0.591703\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Missing values in key columns:')\n",
    "print(df_rq2[['agent_name', 'merged_flag', 'log_total_changes', 'files_changed']].isna().sum())\n",
    "create_divider()\n",
    "\n",
    "print('merged_flag distribution (proportion):')\n",
    "print(df_rq2['merged_flag'].value_counts(normalize=True).round(3))\n",
    "create_divider()\n",
    "\n",
    "agent_summary = (\n",
    "    df_rq2.groupby('agent_name')\n",
    "    .agg(\n",
    "        n_prs=('pr_id', 'nunique'),\n",
    "        mean_total_changes=('total_changes', 'mean'),\n",
    "        median_total_changes=('total_changes', 'median'),\n",
    "        merge_rate=('merged_flag', 'mean'),\n",
    "    )\n",
    "    .sort_values('n_prs', ascending=False)\n",
    ")\n",
    "\n",
    "print('Agent-level summary for RQ2:')\n",
    "print(agent_summary)\n",
    "create_divider()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66505999",
   "metadata": {},
   "source": [
    "### 7. Logistic regression setup\n",
    "\n",
    "Now we model the probability that a PR is merged.\n",
    "\n",
    "Target:\n",
    "\n",
    "- `merged_flag` (1 = merged, 0 = not merged)\n",
    "\n",
    "Features:\n",
    "\n",
    "- Numeric: `log_total_changes`, `files_changed`\n",
    "- Categorical: `agent_name`\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Split into train/test (80/20).\n",
    "2. Build a scikit-learn pipeline with:\n",
    "   - `StandardScaler` for numeric features\n",
    "   - `OneHotEncoder` for `agent_name`\n",
    "   - `LogisticRegression` with `class_weight=\"balanced\"`\n",
    "3. Evaluate with:\n",
    "   - 10-fold stratified cross-validation (ROC AUC)\n",
    "   - Test-set ROC AUC, confusion matrix, and classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f59583bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 26864, Test size: 6716\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define feature and target columns\n",
    "feature_num_cols = ['log_total_changes', 'files_changed']\n",
    "feature_cat_cols = ['agent_name']\n",
    "\n",
    "# Make sure these columns exist\n",
    "for column in feature_num_cols + feature_cat_cols + ['merged_flag']:\n",
    "    assert column in df_rq2.columns, f'Missing column: {column}'\n",
    "\n",
    "X = df_rq2[feature_num_cols + feature_cat_cols]\n",
    "y = df_rq2['merged_flag']\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f'Train size: {len(x_train)}, Test size: {len(x_test)}')\n",
    "create_divider()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0b21ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold CV ROC AUC scores: [0.71810052 0.71641148 0.71209771 0.71361528 0.71502317 0.71532107\n",
      " 0.71422808 0.72443705 0.72054952 0.71207789]\n",
      "Mean CV AUC: 0.716 +/- 0.004\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "numeric_features = feature_num_cols\n",
    "categorical_features = feature_cat_cols\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "log_regression = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "clf = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', log_regression),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 10-fold stratified cross-validation (ROC AUC)\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "cv_auc = cross_val_score(\n",
    "    clf,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "print(f'10-fold CV ROC AUC scores: {cv_auc}')\n",
    "print(f'Mean CV AUC: {cv_auc.mean():.3f} +/- {cv_auc.std():.3f}')\n",
    "create_divider()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "752f1ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test set performance ===\n",
      "Test ROC AUC: 0.717\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[1161  752]\n",
      " [1195 3608]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.61      0.54      1913\n",
      "           1       0.83      0.75      0.79      4803\n",
      "\n",
      "    accuracy                           0.71      6716\n",
      "   macro avg       0.66      0.68      0.67      6716\n",
      "weighted avg       0.73      0.71      0.72      6716\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "=== Coefficients sorted by effect on merge odds ===\n",
      "                   feature      coef  odds_ratio\n",
      "6  agent_name_OpenAI_Codex  0.936027    2.549830\n",
      "4        agent_name_Cursor  0.136502    1.146257\n",
      "1            files_changed  0.047418    1.048560\n",
      "2   agent_name_Claude_Code -0.084469    0.919000\n",
      "0        log_total_changes -0.217280    0.804704\n",
      "5         agent_name_Devin -0.414107    0.660930\n",
      "3       agent_name_Copilot -0.882178    0.413880\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Fit pipeline on training data\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred_proba = clf.predict_proba(x_test)[:, 1]\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print('\\n=== Test set performance ===')\n",
    "print('Test ROC AUC:', round(test_auc, 3))\n",
    "print('\\nConfusion matrix (rows=true, cols=pred):')\n",
    "print(cm)\n",
    "print('\\nClassification report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "create_divider()\n",
    "\n",
    "# Inspect coefficients (feature importance)\n",
    "log_reg_model = clf.named_steps['classifier']\n",
    "ohe = clf.named_steps['preprocessor'].named_transformers_['cat']\n",
    "\n",
    "# Get feature names: numeric + one-hot categorical\n",
    "numeric_names = feature_num_cols\n",
    "cat_ohe_names = ohe.get_feature_names_out(feature_cat_cols).tolist()\n",
    "all_feature_names = numeric_names + cat_ohe_names\n",
    "\n",
    "coef = log_reg_model.coef_[0]\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature': all_feature_names,\n",
    "    'coef': coef,\n",
    "    'odds_ratio': np.exp(coef)\n",
    "}).sort_values('coef', ascending=False)\n",
    "\n",
    "print('=== Coefficients sorted by effect on merge odds ===')\n",
    "print(coef_df)\n",
    "create_divider()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a54b5",
   "metadata": {},
   "source": [
    "### 8. RQ2 conclusion (plain language)\n",
    "\n",
    "**Model performance**\n",
    "\n",
    "- 10-fold stratified CV AUC ‚âà **0.716 ¬± 0.004**\n",
    "- Test-set ROC AUC ‚âà **0.717**\n",
    "\n",
    "This means:\n",
    "\n",
    "- If we randomly pick one merged PR and one non-merged PR,\n",
    "- the model gives the merged one a higher score about **71‚Äì72%** of the time.\n",
    "- So **change complexity + agent identity** carry real signal about merge probability.\n",
    "\n",
    "**Effect of complexity**\n",
    "\n",
    "- `log_total_changes` has a **negative coefficient** (odds ratio ‚âà 0.80).\n",
    "- Interpretation: for a 1 standard deviation increase in log total changes,\n",
    "  the **odds of being merged drop by about 20%**, holding agent identity fixed.\n",
    "- `files_changed` has a very small effect (odds ratio ‚âà 1.05), so once we know how many lines changed, the number of files does not matter much.\n",
    "\n",
    "**Effect of agent identity**\n",
    "\n",
    "After controlling for complexity:\n",
    "\n",
    "- **OpenAI Codex** PRs have much higher merge odds (odds ratio ‚âà 2.55).\n",
    "- **Cursor** has a mild advantage (odds ratio ‚âà 1.15).\n",
    "- **Claude Code** is roughly neutral (odds ratio ‚âà 0.92).\n",
    "- **Devin** PRs have lower merge odds (odds ratio ‚âà 0.66).\n",
    "- **Copilot** has the lowest merge odds (odds ratio ‚âà 0.41).\n",
    "\n",
    "**Answer to RQ2**\n",
    "\n",
    "- **Yes**, change complexity and agent identity both matter.\n",
    "- Larger, more complex PRs are **less likely to be merged**.\n",
    "- Even at similar complexity levels, different AI agents see **different acceptance patterns**: Codex PRs are favored, while Copilot and Devin PRs are more likely to be rejected.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
