{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed6becb4",
   "metadata": {},
   "source": [
    "### RQ2 – Agent identity, change complexity, and merge probability\n",
    "\n",
    "Goal of RQ2:\n",
    "\n",
    "- We want to see **how PR size / complexity** (lines changed, files touched)\n",
    "- and **which AI agent** (Codex, Copilot, Devin, Cursor, Claude Code)\n",
    "- together affect the **probability that a pull request is merged**.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Load the PR table and the commit-detail table.\n",
    "2. Build PR-level complexity features (total lines changed, files changed).\n",
    "3. Join them back to get a `df_rq2` dataset.\n",
    "4. Fit a **logistic regression** model to predict `merged_flag`.\n",
    "5. Use **10-fold cross-validation** + a held-out test set to evaluate performance.\n",
    "6. Interpret coefficients to answer RQ2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c67d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# scikit-learn core tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Just for nicer console output\n",
    "def create_divider():\n",
    "    print('-' * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de508e3",
   "metadata": {},
   "source": [
    "### 1. Load main pull request table\n",
    "\n",
    "First, we load the main `pull_request.parquet` table.  \n",
    "This gives us PR-level metadata:\n",
    "\n",
    "- PR id, agent name, state (open/closed), timestamps, repo id, etc.\n",
    "- It **does not** contain complexity features like additions/deletions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e68990",
   "metadata": {},
   "outputs": [],
   "source": [
    "pull_requests = pd.read_parquet(\n",
    "    'hf://datasets/hao-li/AIDev/pull_request.parquet'\n",
    ")\n",
    "\n",
    "print('pull_requests shape:', pull_requests.shape)\n",
    "create_divider()\n",
    "print('pull_requests columns:')\n",
    "print(pull_requests.columns)\n",
    "create_divider()\n",
    "\n",
    "print(pull_requests.head())\n",
    "create_divider()\n",
    "pull_requests.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaf9632",
   "metadata": {},
   "source": [
    "### 2. The PR table is missing complexity columns\n",
    "\n",
    "The main PR table does **not** have:\n",
    "\n",
    "- `additions`\n",
    "- `deletions`\n",
    "- `files_changed`\n",
    "\n",
    "These fields live in the commit-related tables.  \n",
    "To build PR-level complexity, we now load:\n",
    "\n",
    "- `pr_commits.parquet`\n",
    "- `pr_commit_details.parquet`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e53890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a working copy of the pull_request table\n",
    "prs = pull_requests.copy()\n",
    "\n",
    "# Just to verify these features do NOT exist in this table\n",
    "key_cols = ['pr_id', 'agent_name', 'additions', 'deletions', 'files_changed']\n",
    "for c in key_cols:\n",
    "    print(c, 'in columns?', c in prs.columns)\n",
    "\n",
    "create_divider()\n",
    "\n",
    "# Load commit-level tables\n",
    "pr_commits = pd.read_parquet(\n",
    "    'hf://datasets/hao-li/AIDev/pr_commits.parquet'\n",
    ")\n",
    "\n",
    "pr_commit_details = pd.read_parquet(\n",
    "    'hf://datasets/hao-li/AIDev/pr_commit_details.parquet'\n",
    ")\n",
    "\n",
    "print('pr_commits columns:')\n",
    "print(pr_commits.columns)\n",
    "create_divider()\n",
    "\n",
    "print('pr_commit_details columns:')\n",
    "print(pr_commit_details.columns)\n",
    "create_divider()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0abcaf",
   "metadata": {},
   "source": [
    "### 3. Clean PR table and define merge flag\n",
    "\n",
    "We standardize column names and create a binary target:\n",
    "\n",
    "- `pr_id` – PR identifier\n",
    "- `agent_name` – which AI agent created this PR\n",
    "- `merged_flag` – 1 if `merged_at` is non-null, 0 otherwise\n",
    "\n",
    "`merged_flag` is our **target variable** for logistic regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee18e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs = prs.rename(columns={\n",
    "    'id': 'pr_id',        # PR identifier\n",
    "    'agent': 'agent_name' # AI agent\n",
    "})\n",
    "\n",
    "# Merge flag: 1 if merged_at is not null, else 0\n",
    "prs['merged_flag'] = prs['merged_at'].notna().astype(int)\n",
    "\n",
    "print('=== PR-level columns (subset) ===')\n",
    "print(prs[['pr_id', 'agent_name', 'state', 'created_at', 'merged_at', 'merged_flag']].head())\n",
    "\n",
    "create_divider()\n",
    "print('merged_flag value proportions:')\n",
    "print(prs['merged_flag'].value_counts(normalize=True).round(3))\n",
    "create_divider()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965363d2",
   "metadata": {},
   "source": [
    "### 4. Build PR-level complexity features\n",
    "\n",
    "Now we use `pr_commit_details` to build complexity for each PR.\n",
    "\n",
    "For each `pr_id`, we compute:\n",
    "\n",
    "- `total_additions` – sum of additions across all files\n",
    "- `total_deletions` – sum of deletions\n",
    "- `files_changed` – number of unique files touched\n",
    "- `total_changes` – sum of `changes` across files (lines changed)\n",
    "\n",
    "These become our **complexity features**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d342a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work from pr_commit_details\n",
    "cd = pr_commit_details.copy()\n",
    "\n",
    "print('=== commit_details sample ===')\n",
    "print(cd.head())\n",
    "create_divider()\n",
    "\n",
    "# Aggregate per PR\n",
    "complexity = (\n",
    "    cd.groupby('pr_id')\n",
    "      .agg(\n",
    "          total_additions=('additions', 'sum'),\n",
    "          total_deletions=('deletions', 'sum'),\n",
    "          files_changed=('filename', 'nunique'),\n",
    "          total_changes=('changes', 'sum'),\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "print('=== Complexity features (per PR) ===')\n",
    "print(complexity.head())\n",
    "create_divider()\n",
    "\n",
    "# Replace any remaining NaNs with 0 (safe for these counts)\n",
    "complexity[['total_additions', 'total_deletions', 'files_changed', 'total_changes']] = (\n",
    "    complexity[['total_additions', 'total_deletions', 'files_changed', 'total_changes']]\n",
    "    .fillna(0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e3cf98",
   "metadata": {},
   "source": [
    "### 5. Check missing values in PR table\n",
    "\n",
    "Some columns in `prs` have missing values, e.g. `merged_at` and `closed_at`.\n",
    "This is **expected**:\n",
    "\n",
    "- `merged_at` is missing when a PR was never merged.\n",
    "- We already encoded this into `merged_flag`.\n",
    "\n",
    "For RQ2, we only care that:\n",
    "\n",
    "- `pr_id`, `agent_name`, `merged_flag` are not missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459ae4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing values in prs:')\n",
    "print(prs.isna().sum().sort_values(ascending=False))\n",
    "create_divider()\n",
    "\n",
    "print('Proportion missing per column (%):')\n",
    "print((prs.isna().mean() * 100).round(2).sort_values(ascending=False))\n",
    "create_divider()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02872075",
   "metadata": {},
   "source": [
    "### 6. Merge PR metadata with complexity and create log features\n",
    "\n",
    "We now join:\n",
    "\n",
    "- `prs` (PR metadata + agent + merged_flag)\n",
    "- `complexity` (per-PR size metrics)\n",
    "\n",
    "Then:\n",
    "\n",
    "- Fill missing `total_changes` and `files_changed` with 0 (for safety).\n",
    "- Create `log_total_changes = log(1 + total_changes)` to reduce skew.\n",
    "- Drop rows where key variables are missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893254e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge PR metadata with complexity features\n",
    "df_rq2 = prs.merge(complexity, on='pr_id', how='inner')\n",
    "\n",
    "print('df_rq2 shape:', df_rq2.shape)\n",
    "create_divider()\n",
    "print(df_rq2[['pr_id', 'agent_name', 'merged_flag', 'total_changes', 'files_changed']].head())\n",
    "create_divider()\n",
    "\n",
    "# Basic cleaning\n",
    "df_rq2['total_changes'] = df_rq2['total_changes'].fillna(0)\n",
    "df_rq2['files_changed'] = df_rq2['files_changed'].fillna(0)\n",
    "\n",
    "# Log-transform complexity\n",
    "df_rq2['log_total_changes'] = np.log1p(df_rq2['total_changes'])\n",
    "\n",
    "# Drop any rows missing in model-relevant columns\n",
    "df_rq2 = df_rq2.dropna(\n",
    "    subset=['agent_name', 'merged_flag', 'log_total_changes', 'files_changed']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4592a2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing values in key columns:')\n",
    "print(df_rq2[['agent_name', 'merged_flag', 'log_total_changes', 'files_changed']].isna().sum())\n",
    "create_divider()\n",
    "\n",
    "print('merged_flag distribution (proportion):')\n",
    "print(df_rq2['merged_flag'].value_counts(normalize=True).round(3))\n",
    "create_divider()\n",
    "\n",
    "agent_summary = (\n",
    "    df_rq2.groupby('agent_name')\n",
    "    .agg(\n",
    "        n_prs=('pr_id', 'nunique'),\n",
    "        mean_total_changes=('total_changes', 'mean'),\n",
    "        median_total_changes=('total_changes', 'median'),\n",
    "        merge_rate=('merged_flag', 'mean'),\n",
    "    )\n",
    "    .sort_values('n_prs', ascending=False)\n",
    ")\n",
    "\n",
    "print('Agent-level summary for RQ2:')\n",
    "print(agent_summary)\n",
    "create_divider()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66505999",
   "metadata": {},
   "source": [
    "### 7. Logistic regression setup\n",
    "\n",
    "Now we model the probability that a PR is merged.\n",
    "\n",
    "Target:\n",
    "\n",
    "- `merged_flag` (1 = merged, 0 = not merged)\n",
    "\n",
    "Features:\n",
    "\n",
    "- Numeric: `log_total_changes`, `files_changed`\n",
    "- Categorical: `agent_name`\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Split into train/test (80/20).\n",
    "2. Build a scikit-learn pipeline with:\n",
    "   - `StandardScaler` for numeric features\n",
    "   - `OneHotEncoder` for `agent_name`\n",
    "   - `LogisticRegression` with `class_weight=\"balanced\"`\n",
    "3. Evaluate with:\n",
    "   - 10-fold stratified cross-validation (ROC AUC)\n",
    "   - Test-set ROC AUC, confusion matrix, and classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59583bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature and target columns\n",
    "feature_num_cols = ['log_total_changes', 'files_changed']\n",
    "feature_cat_cols = ['agent_name']\n",
    "\n",
    "# Make sure these columns exist\n",
    "for column in feature_num_cols + feature_cat_cols + ['merged_flag']:\n",
    "    assert column in df_rq2.columns, f'Missing column: {column}'\n",
    "\n",
    "X = df_rq2[feature_num_cols + feature_cat_cols]\n",
    "y = df_rq2['merged_flag']\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f'Train size: {len(x_train)}, Test size: {len(x_test)}')\n",
    "create_divider()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b21ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "numeric_features = feature_num_cols\n",
    "categorical_features = feature_cat_cols\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "log_regression = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "clf = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', log_regression),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 10-fold stratified cross-validation (ROC AUC)\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "cv_auc = cross_val_score(\n",
    "    clf,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "print(f'10-fold CV ROC AUC scores: {cv_auc}')\n",
    "print(f'Mean CV AUC: {cv_auc.mean():.3f} +/- {cv_auc.std():.3f}')\n",
    "create_divider()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752f1ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit pipeline on training data\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred_proba = clf.predict_proba(x_test)[:, 1]\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print('\\n=== Test set performance ===')\n",
    "print('Test ROC AUC:', round(test_auc, 3))\n",
    "print('\\nConfusion matrix (rows=true, cols=pred):')\n",
    "print(cm)\n",
    "print('\\nClassification report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "create_divider()\n",
    "\n",
    "# Inspect coefficients (feature importance)\n",
    "log_reg_model = clf.named_steps['classifier']\n",
    "ohe = clf.named_steps['preprocessor'].named_transformers_['cat']\n",
    "\n",
    "# Get feature names: numeric + one-hot categorical\n",
    "numeric_names = feature_num_cols\n",
    "cat_ohe_names = ohe.get_feature_names_out(feature_cat_cols).tolist()\n",
    "all_feature_names = numeric_names + cat_ohe_names\n",
    "\n",
    "coef = log_reg_model.coef_[0]\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature': all_feature_names,\n",
    "    'coef': coef,\n",
    "    'odds_ratio': np.exp(coef)\n",
    "}).sort_values('coef', ascending=False)\n",
    "\n",
    "print('=== Coefficients sorted by effect on merge odds ===')\n",
    "print(coef_df)\n",
    "create_divider()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a54b5",
   "metadata": {},
   "source": [
    "### 8. RQ2 conclusion (plain language)\n",
    "\n",
    "**Model performance**\n",
    "\n",
    "- 10-fold stratified CV AUC ≈ **0.716 ± 0.004**\n",
    "- Test-set ROC AUC ≈ **0.717**\n",
    "\n",
    "This means:\n",
    "\n",
    "- If we randomly pick one merged PR and one non-merged PR,\n",
    "- the model gives the merged one a higher score about **71–72%** of the time.\n",
    "- So **change complexity + agent identity** carry real signal about merge probability.\n",
    "\n",
    "**Effect of complexity**\n",
    "\n",
    "- `log_total_changes` has a **negative coefficient** (odds ratio ≈ 0.80).\n",
    "- Interpretation: for a 1 standard deviation increase in log total changes,\n",
    "  the **odds of being merged drop by about 20%**, holding agent identity fixed.\n",
    "- `files_changed` has a very small effect (odds ratio ≈ 1.05), so once we know how many lines changed, the number of files does not matter much.\n",
    "\n",
    "**Effect of agent identity**\n",
    "\n",
    "After controlling for complexity:\n",
    "\n",
    "- **OpenAI Codex** PRs have much higher merge odds (odds ratio ≈ 2.55).\n",
    "- **Cursor** has a mild advantage (odds ratio ≈ 1.15).\n",
    "- **Claude Code** is roughly neutral (odds ratio ≈ 0.92).\n",
    "- **Devin** PRs have lower merge odds (odds ratio ≈ 0.66).\n",
    "- **Copilot** has the lowest merge odds (odds ratio ≈ 0.41).\n",
    "\n",
    "**Answer to RQ2**\n",
    "\n",
    "- **Yes**, change complexity and agent identity both matter.\n",
    "- Larger, more complex PRs are **less likely to be merged**.\n",
    "- Even at similar complexity levels, different AI agents see **different acceptance patterns**: Codex PRs are favored, while Copilot and Devin PRs are more likely to be rejected.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
